{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Toolkit (NLTK)\n",
    "\n",
    "* [NLTK Documentation](https://www.nltk.org/#)<br/>\n",
    "* [Natural Language Processing with Python (The NLTK book)](https://www.nltk.org/book/)<br/>\n",
    "* [Project Gutenberg](https://www.gutenberg.org/)<br/>\n",
    "* [Regular Expressions Operations in Python](https://docs.python.org/3/library/re.html)\n",
    "* [Regular Expressions Tutorial at Regexone.com](https://regexone.com/)\n",
    "* [Link to list of Part Of Speech (POS) tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "\n",
    "<blockquote>\n",
    "<p>NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.</p>\n",
    "\n",
    "<p>Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.</p>\n",
    "<footer style=\"text-align:right;\">Natural Language Toolkit â€” NLTK 3.3 documentation, NLTK Project, accessed 2018/10/01, https://www.nltk.org/# </footer>\n",
    "</blockquote>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you installed Anaconda you will have most of the NLTK resources but may still need to download some. The code below will bring you to the python downloader. From here you can download NLTK resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download NLTK resources\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you know precisely what resource you need you can also directly download it.\n",
    "# Access the resource in the download function by using the resources name.\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization in NLTK is the process of taking a string and breaking it into smaller meaningful pieces (e.g. words, or sentences). Very generally this works by seperating long strings of text into a lisst of smaller strings that represent words or sentences. \n",
    "\n",
    "Python has it's own built in function for doing this, the `split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will take the sentence below and split it into a string of words.\n",
    "# we will use whitespace to indicate the break points.\n",
    "text = \"I went to Bymarka and hiked all the way to the top.  The weather wasn't great.\"\n",
    "\n",
    "print(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we want to save this we can store it in a variable\n",
    "tokens = text.split(' ')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, notice we have a blank space in the text in between the sentences. It looks like there may have been a style to use two white space characters between sentences. To make our split a little smarter we can use regular expressions in python if we import the regular expressions module.<br/>\n",
    "* [Regular Expressions Operations in Python](https://docs.python.org/3/library/re.html)\n",
    "* [Regular Expressions Tutorial at Regexone.com](https://regexone.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tokens = re.split('\\s+',text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could refine our regular expression even further but NLTK gives us a much simpler way to tokenize a string of text. NLTK has its own tokenizing functions for words and sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This wil return a list of the sentences in our string. \n",
    "sent_tokens = sent_tokenize(text)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return a list of tokens as words\n",
    "word_tokens = word_tokenize(text)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the words, and the punctuation and the extra space has been removed. However we have some trouble with the contraction \"wasn't\". Let's look at some other ways to tokenize this and see if we can get fine tune this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to get more information on how these work you can use the jupyter notebooks `?` for help\n",
    "nltk.tokenize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers\n",
    "NLTK has several tokenizers that work in different ways. \n",
    "* **WhiteSpaceTokenizer** will simply tokenize based on white space. This is very much like the split function we used earlier.\n",
    "* **PunktSentTokenizer** based on Tibor Kiss and Jan Stunk's work on boundar detection. This tokenizer splits on punctuation and does not separate it from the word. \n",
    "* **WordPunctTokenizer** will split all puntuation into its own token.\n",
    "* ***TreebankWordTokenizer*** uses the [Penn Tree bank corpus](https://link.springer.com/chapter/10.1007/978-94-010-0201-1_1) to determine where to break up the string.\n",
    "* **RegexpTokenizer** will work with a regular expression pattern you create. This is useful if you need to create a highly idiomatic tokenizer. The advantage is it can be tailored to the content however you should be confident in you understanding of regular expressions to identify patterns. [Regex Cheetsheet](https://www.rexegg.com/regex-quickstart.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we look at the documentaiton we see this is the same thing as using sent_tokenize\n",
    "nltk.tokenize.sent_tokenize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the standard tokenizers treated the contraction quite how we would like to see it. Perhaps we need to use the Regular Expression tokenizer to deal with this special case. Regular Expressions can be tricky so it is important to understand how these work when applying them to your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a standard expression for splitting a string into word tokens.\n",
    "tokenizer = RegexpTokenizer(\"[A-Za-z,']+|[^\\w\\s]\")\n",
    "word_tokens = tokenizer.tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "Stop words are words that do not add any meaning to the text. And we will see later that they can often introduce \"noise\" into the analysis of a text or corpus. A common practice in Natural Language Processing is to remove stop words from a text. The purpose of removing stopwords is that only words that can provide useful insight into the text will remain. Stop words are words like \"the\", \"a\", \"it\". NLTK gives us an easy way to remove stop words form a text by using a corpus called `stopwords`. NLTK also provides stopwords in many languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our text we will want to use enlgish. We could use them all by not passing in a language, i.e. `stopwords.words()`\n",
    "# To see the languages available we examine the file ids.\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_eng = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to remove stop words from a text we need to create a \"list comprehension\". A list comprehension is just a simple way to create a list using an expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have our text tokenized in `word_tokens`\n",
    "[word for word in word_tokens if word not in stopwords_eng]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we kept the word \"The\". The capitalization is a factor in determining stop words. Let's adjust the comprehension to take this into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word for word in word_tokens if word.lower() not in stopwords_eng]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see all of the stop words have been remove, regardless of capitalization. Let's put this list into a new variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [word for word in word_tokens if word.lower() not in stopwords_eng]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemma and Stemming\n",
    "Stemming is essentially a crude version of lemmatization. Both are attempts at unifying variations of one word to a single source. In NLTK stemming uses an algorithm to basically cut off the tail end of a word. Stemming will turn \"cats\" into \"cat\" so that all the instances of the word \"cat\" can be identified.\n",
    "\n",
    "Lemmatization however relies on more semantic methods to derive the lemma of a word using WordNet corpus as a lexical resource. The lemma is akin to what you would find in a dictionary. The lemma for \"wolves\" would be \"wolf\".\n",
    "\n",
    "We will look more closely at WordNet later. For the moment lets see how these two different means of reduction affect the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "NLTK provides several different stemmer algorithms.\n",
    "* **PorterStemmer** relies on the [Porter Stemming algorithm](https://tartarus.org/martin/PorterStemmer/) developed by Martin Porter.\n",
    "* **LancasterStemmer**  A word stemmer based on the Lancaster(Paice/Husk) stemming algorithm.\n",
    "* **SnowballStemmer** An algorithm based on the [Snowball stemming algorithm](http://snowballstem.org/) also developed by Martin Porter.\n",
    "\n",
    "You can see a [complete list of the different stemmers available in the nltk.stem API on nltk.org's documentation](https://www.nltk.org/api/nltk.stem.html?highlight=lancaster%20stemmer#nltk.stem.lancaster.LancasterStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "porter_stemmer.stem('knives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lancaster_stemmer.stem('knives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "snowball_stemmer.stem('Knives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "NLTK relies on the WordNet corpus to identify the lemma of a word. The WordNet Corpus is a lexical resource that has tagged and correlated speech with a great degree of granularity. We will look further into it in a bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('knives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun let's right a function that will quickly show us all of the different ways these stemmers/lemmatizers will  treat a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_compare(word):\n",
    "    print('Lemma:          ',lemmatizer.lemmatize(word) )\n",
    "    print('Porter Stem:    ', porter_stemmer.stem(word))\n",
    "    print('Lancaster Stem: ', lancaster_stemmer.stem(word))\n",
    "    print('Snowball Stem:  ', snowball_stemmer.stem(word))\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_compare('mystery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can even use this on our list of data.\n",
    "t = [stem_compare(w) for w in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation\n",
    "\n",
    "Collocations are words which commonly occur together in a text or corpus. For the moment we will look specifically at Bigrams and Trigrams, although there is no limit to the number of words that Python could evaluate for the likelihood of collocation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigrams and trigrams\n",
    "\n",
    "To find bigrams in a text that frequently appear together we can use the BigramCollocationFinder. This will help to identify word pairs like \"hot dog\", \"hanky panky\", \"a lot\", \"no one\". A trigram is the same thing but instead of finding two words that frequently appear together it locates groups of three.<br/>\n",
    "For this we will need a larger text. NLTK has these available to us in its corpus module. We will look at this is greater detail later. For the moment, we will use it to import the text of Lewis Carrol's Alice in Wonderland from [Project Guteneberg](https://www.gutenberg.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wonderland = gt.raw('carroll-alice.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wonderland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_words = tokenizer.tokenize(wonderland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add punctuation to the stoplist words\n",
    "import string\n",
    "punct = string.punctuation\n",
    "punct = [l for l in punct]\n",
    "stopwords_eng = stopwords_eng + punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_alice = [word for word in alice_words if word.lower() not in stopwords_eng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(core_alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigrams\n",
    "\n",
    "Notice the different scoring measures for the bigrams. For more detailson bigram scoring and the api see the [NLTK api metrics documentation](https://www.nltk.org/api/nltk.metrics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = BigramCollocationFinder.from_words(core_alice, window_size=2) \n",
    "bigrams.nbest(BigramAssocMeasures.likelihood_ratio, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.nbest(BigramAssocMeasures.raw_freq, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.nbest(BigramAssocMeasures.pmi, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.nbest(BigramAssocMeasures.fisher, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look a the score for a given bigram\n",
    "bigrams.score_ngram(BigramAssocMeasures.likelihood_ratio, 'Mock','Turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.score_ngram(BigramAssocMeasures.raw_freq, 'said', 'Alice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.score_ngram(BigramAssocMeasures.likelihood_ratio, \"'Back\", 'land')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.score_ngram(BigramAssocMeasures.fisher, \"',\", 'holding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that this score may reflect bigrams that have words that have a high collocation but are used infrequently. For example, 'hot dog' might only appear twice in a text but the collocation score will be high because the word 'hot' only ever appears with the word 'dog'. While the two words are higly associative they may not be particularly relevant to a text's meaning. In order to take this into account we can also factor in the frequencyof the bigram in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BigramAssocMeasures.pmi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = BigramCollocationFinder.from_words(core_alice, window_size=2) \n",
    "bigrams.apply_freq_filter(5)\n",
    "bigrams.nbest(BigramAssocMeasures.pmi, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = BigramCollocationFinder.from_words(core_alice, window_size=2) \n",
    "bigrams.apply_freq_filter(10)\n",
    "bigrams.nbest(BigramAssocMeasures.likelihood_ratio, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BigramAssocMeasures.likelihood_ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = BigramCollocationFinder.from_words(core_alice, window_size=2) \n",
    "bigrams.apply_freq_filter(3)\n",
    "bigrams.nbest(BigramAssocMeasures.fisher, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = BigramCollocationFinder.from_words(core_alice, window_size=2) \n",
    "bigrams.apply_freq_filter(10)\n",
    "bigrams.nbest(BigramAssocMeasures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Looks like we might need to work on the tokenization. The punctuation is throwing things off a bit. Perhaps we want to try another tokenizer. One feature of the corpus reader is that it has already tokenized the text. Here we used the raw version but we can import the corpus' tokenized version of the file with `gt.words('carroll-alice.txt')`</p>\n",
    "<p>Try using that version of the file and see what results you get.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wonderland = gt.words('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wonderland =[w for w in wonderland if w.lower() not in stopwords_eng]\n",
    "wonderland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigrams\n",
    "Trigrams are executed almost exactly the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder \n",
    "from nltk.metrics import TrigramAssocMeasures \n",
    "trigrams = TrigramCollocationFinder.from_words(wonderland,window_size=5)\n",
    "trigrams.nbest(TrigramAssocMeasures.likelihood_ratio,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams.nbest(TrigramAssocMeasures.raw_freq,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams.nbest(TrigramAssocMeasures.pmi,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and again we can see the score for a trigram:\n",
    "trigrams.score_ngram(TrigramAssocMeasures.likelihood_ratio, 'said','Mock','Turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams.score_ngram(TrigramAssocMeasures.likelihood_ratio, 'Gryphon','Mock','Turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech Tagging (POS)\n",
    "Tagging is a way of identifying certain properties of words, or tokens, in NLTK. For example, we could identify verbs and nouns in a text by tagging them with a string that indicates this semantic meaning. Identifying the proper parts of speech is complicated in a text, particularly if there are many unknown, or new uses of words, or the text is highly idiosyncratic (like Twitter messages). However, NLTK does include a recommender speech tagger. \n",
    "\n",
    "* [Link to list of Part Of Speech (POS) tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get sme new text to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Derek is working at Harvard Business Publishing and walked around Paris for five days.'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the tagger to work we must tokenize the sentence into individual words. Let's use the nltk tokenizer and keep our lives simple for the moment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text = word_tokenize(text)\n",
    "token_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "pos = pos_tag\n",
    "pos_text = pos(token_text)\n",
    "print(pos_text)\n",
    "# Note that the word, POS tag pairs are in parentheses \"()\", denoting these are tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|POS Tag|Description|\n",
    "|---|-----------|\n",
    "|NNP|Proper Noun Singular |\n",
    "|NNS| Noun Plural\n",
    "|CC | Coordinating Conjunction |\n",
    "|CD | Cardinal Number |\n",
    "|IN| Preposition, or subordinating conjunction |\n",
    "|VBD| Verb Past Tense  |\n",
    "|VBZ| 3rd person singular present |\n",
    "|VBG|\tVerb, gerund or present participle |\n",
    "|RP | Particle |\n",
    "|.  | Punctuation |\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "NLTK also has the ability to recognize names in a text using a \"chunker\" to identify a list of tagged tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note when running the name entity recognition you may need to down load \"ghostscript\" \n",
    "# Mac users can use 'brew install ghostscript'. For others a Google search should do the trick.  \n",
    "# this script is used to draw a tree of the named entities\n",
    "from nltk import ne_chunk\n",
    "names = ne_chunk(pos_text)\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export this graphic\n",
    "# using the draw function will create an external tree that you can save as a postscript file.\n",
    "nltk.ne_chunk(pos_text).draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
