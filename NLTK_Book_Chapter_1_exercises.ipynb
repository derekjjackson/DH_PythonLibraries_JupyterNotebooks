{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the NLTK book collection and some basic statistical analysis\n",
    "\n",
    "This notebook is based on the exercises described in [NLTK Book, Chapter 1: Language Processing and Python](https://www.nltk.org/book/ch01.html)\n",
    "<footer style=\"text-align:right;font-size:.8em;\">Source: Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. Oâ€™Reilly Media Inc. http://nltk.org/book</footer>\n",
    "\n",
    "* [NLTK documentation](https://www.nltk.org/)\n",
    "* [NLTK API Documentaiton](https://www.nltk.org/api/nltk.html)\n",
    "* [PyCharm](https://www.jetbrains.com/pycharm/download/#section=mac) An Integrated Data Environment (IDE) for working with Python. Has a greater focus on writing python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the Book Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at the downloader and all the tools/packages available.\n",
    "Then let's download the \"book\" collection.\n",
    "\n",
    "***Note:*** **This interface will look significantly different in the browser notebook and a local installation. If you are running the notebook on your computer a separate download GUI may load.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the nice message about the `texts()` and `sents()` functions. Let's try those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of the texts\n",
    "texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first sentence to each text\n",
    "sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at individual text info.\n",
    "text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this is a special NLTK data type. This is signifanct because it will have it's own unique methods and functions, which we will explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concordance\n",
    "\n",
    "Concordance is one of the special functions available in the books module.\n",
    "`concordance` will return a string of results showing all the occurrences of a word and the context in which the word appears. This is also known as a KWIC (Key Word in Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance('alone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The default view is limited to 25 rows but you can change that to show more or less by passing in an integer for the number of lines** `lines=n` \n",
    "**Similarly we can dictate the width of the line (in characters):** `width=n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance('death', width=40, lines=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Multiple Texts\n",
    "texts = [text1,text2,text5]\n",
    "\n",
    "for t in texts:\n",
    "    print (t)\n",
    "    t.concordance('red',lines=10)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find words that are used similarly with the `similar` function. This function will show all of the words that are used in a similar fashion (as determined by NLTK). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar usages of 'red' in Moby Dick\n",
    "print(text1)\n",
    "text1.similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar usages to 'red' in Sense and Sensibility\n",
    "print(text2)\n",
    "text2.similar('love')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at similar ways that similar words are used. `common_contexts` will show how the words are used in similar ways. Compare words from the list of words returned from the similar function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.common_contexts([\"love\",\"affection\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.common_contexts(['love','sea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dispersion\n",
    "see a dispersion plot of the word in the context of the entire text.\n",
    "\n",
    "note: matplotlib is a python library for plotting and visualizing data. We will look at it more later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "text1.dispersion_plot(['blue','red','green','yellow','gold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a closer look at an area of the text\n",
    "text1[150000:210000].count('gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"gold\" appears 16 times in this region of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance('gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other analysis tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the length of a text, including words, and punctuation, i.e. all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is distinct from a word list of all the words that appear in the text (i.e. not inlcuding repeated words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Genesis is made up of 2789 unique tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how ***\"lexically rich\"*** a text is by dividing the number of words used by the number of words in the text. This will give us an idea of how often words are used in a text. We git a percentage that will indicate what percentage of the words used in the entire text are distince. In other words, does the text use the same words repeatedly, or does it have a rich vocabulary and use many different words through out the text.\n",
    "\n",
    "This can be a little misleading when thinking about text of different size. There is a finite vocabulary to introduce into a text, while there is an infinite possibility to a text lengths. A text that is 1000 words long and uses only 100 words in its vocabulary, would have the same percentage as a text that is 100,000, but uses a 10,000 word vocabulary. Can we really say that they both have the same lexical diversity. To make comparisons about lexical diversity the texts need to be of similar lengths. There are other mathematical ways to normalize this value as well and this is where we would like the input of a data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show that the number of distinct words is just 6% of the total text. \n",
    "(len(set(text3))/len(text3)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### looking at particular words\n",
    "We can take a closer look at particular words in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often does a word appear in a text. Keep in mind we have not done anything to the text, so searching for 'lol', is not the same as searching for ('LOL') in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text5.count('lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show us that 'lol' is used in 1.5% of the text.\n",
    "text5.count('lol')/len(text5) *100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we missed some LOLs... (lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text5.count('LOL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save our self some time by turning these into reusable functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_div(text):\n",
    "    return (len(set(text))/len(text)) * 100\n",
    "    \n",
    "def wrd_freq(text,word):\n",
    "    return text.count(word)/len(text) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_div(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrd_freq(text5,'lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to find and return all the locations in the text of a particular word.\n",
    "def find(word,text):\n",
    "    for x in range(len(text)):\n",
    "        if text[x] == word:\n",
    "            print (word + ': ' + str(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find('Ishmael',text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance('Ishmael')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a way to locate the concordances in the text. The first concordance should correspond to the first index position given in our function. \n",
    "\n",
    "*Note: we may need to do some fine tuning for other words. Our function does not take cpaitalization into account while the concordance function does. Our function would not show us \"ishmael\" if it appeared in the text, but the concordance would.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(text1[4712:4750]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### putting words into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first sentence of Moby Dick\n",
    "sent = text1[4712:4716]\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can apply our function for lexical diversity to a list as well. \n",
    "# Every word will be uniques so we should get a 100%\n",
    "lex_div(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember a few sentences are predefined with the book collection we imported. sent1-sent9\n",
    "sent1, sent2, sent3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list and try using some of the functions we used earlier on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1 = ['It','is','only','a','scratch','!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(sen1)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make vocab a list\n",
    "list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1.count('scratch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use python addition operators to concatenate lists as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen2 = ['I','did','not'] + ['vote','for','you']\n",
    "sen2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I forgot the period append will add an item to the end of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a period to the end of the list\n",
    "sen2.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List index\n",
    "every item in a list has an index number associated with it. The index starts at 0, so to find the first item in a list we can refer to the index position '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the reverse and get the index position of a word in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1.index('scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more than one item from the list we can use : notation to indicate a span of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1[4712:4716]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start from the beginning we do not need to refer to the index\n",
    "text1[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or the end\n",
    "text1[260800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire text\n",
    "text1[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can recreate the original string (sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = \" \".join(sen1)\n",
    "string1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that strings also can take some of the functions we have been using on lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing in some Statistics and Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution\n",
    "\n",
    "We can use this method to help determine the importance of words. The idea being that words used more frequently will be more important. `FreqDist` will create an index of all the words used in the text (samples) and record how many times each word, or sample, appears in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fq_dis1 =FreqDist(text1)\n",
    "print(fq_dis1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a large collection, 19317 samples. Looking at it all is daunting but do we really need to see all of it.\n",
    "\n",
    "Note: Outcomes should equal the number of tokens in the text. `len(text1)` Let's check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will show us the words used most frequently. We can choose the range.\n",
    "# so to see the top 50 we sue:\n",
    "fq_dis1.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the frequency of a particular word:\n",
    "fq_dis1['Ishmael']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the frequency of a word. \n",
    "fq_dis1.freq('Ishmael')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot this two ways. The first way we see the individual values of each word. In the second we see how the word counts accumulate towards the total word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq_dis1.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the frequency distribution:\n",
    "fq_dis1.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of these words are not helpful in understanding the text. \"whale\" might be the only telling word in our frequency distribution. You can also look at the **hapaxes** or the words that only appear once to see if these are more insightful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq_dis1.hapaxes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rare words don't offer much insight either, in this case. It looks like we will need to be more selective of what words in the text we want to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try removing stop words from the text as we saw in the earlier notebook and then measure the frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### long words\n",
    "\n",
    "Can we glean anything from looking at the longer words in the text. In python this is written: `word for word in words if len(word)>n` where `n` is the word length we want to examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicates first by creating a set of the text. \n",
    "# Our iteration returns a list and so our result will return duplicates if they exist. \n",
    "# We should also put the list into a set to remove duplicates.\n",
    "\n",
    "words_10 = set([word for word in text1 if len(word) > 10])\n",
    "sorted(words_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting but still not very insightful. However with other texts this might not be the case. look at other texts and see if the word length reveals anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_15 = set([word for word in text5 if len(word) > 15])\n",
    "sorted(words_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_15 = set([word for word in text4 if len(word) > 10])\n",
    "sorted(words_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine these features (frequency distribution and word length) to get another look into the text. Let's try the chat text and find words that are 8 characters or longer and are found more than 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq_dis5=FreqDist(text5)\n",
    "sorted(word for word in set(text5) if len(word) > 8 and fq_dis5[word] >5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What words was used the most in texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning this will be dissapointing\n",
    "fq_dis5.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq_dis5.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams and collocation\n",
    "**Collocation** is a sequence of words that occur together with some frequency that is statistically significant. We also want to focus on collocation that has meaning. e.g. 'dark chocolate' and not 'the chocolate'\n",
    "\n",
    "**Bigram** is a a pair of consecutive words, e.g. \"hot tub\". \n",
    "\n",
    "For bigrams we have a function in NLTK. the bigrams function takes a Python list evaluates it as word pairs. We can then store the bigrams in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "bgram = list(bigrams(['more', 'is', 'said', 'than', 'done']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can get all of the bigrams for a text:\n",
    "list(set(bigrams(text1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where collocation is useful. The collocation function will essentially give us a list of bigrams that are statistically relevant because the collocations contain rare words that appear with other words at a higher frequency than is statistically expected. This is basically the same thing we did earlier to create bigrams but it is a function of the book module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder: text8 are the Personals\n",
    "text8.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ways of Counting\n",
    "\n",
    "We can also Count the words based on word length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first get a list that contains all the words' length.\n",
    "ls = [len(word) for word in text1]\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now get the freq distribution of that list:\n",
    "fq_dist_wordlength = FreqDist(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq_dist_wordlength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the distribution with the highest frequnecy\n",
    "fq_dist_wordlength.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq_dist_wordlength[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the top 10\n",
    "fq_dist_wordlength.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the percentage of frequency of 3 letter words?\n",
    "fq_dist_wordlength.freq(3) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total number of samples\n",
    "# again this should be the same as the word count of the text.\n",
    "fq_dist_wordlength.N() == len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq_dist_wordlength.plot(cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some quick observations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### additional Word (string data types) operators \n",
    "`s.startswith(t)`\ttest if s starts with t\n",
    "\n",
    "`s.endswith(t)`\ttest if s ends with t\n",
    "\n",
    "`t in s`\ttest if t is a substring of s\n",
    "\n",
    "`s.islower()`\ttest if s contains cased characters and all are lowercase\n",
    "\n",
    "`s.isupper()`\ttest if s contains cased characters and all are uppercase\n",
    "\n",
    "`s.isalpha()`\ttest if s is non-empty and all characters in s are alphabetic\n",
    "\n",
    "`s.isalnum()`\ttest if s is non-empty and all characters in s are alphanumeric\n",
    "\n",
    "`s.isdigit()`\ttest if s is non-empty and all characters in s are digits\n",
    "\n",
    "`s.istitle()`\ttest if s contains cased characters and is titlecased (i.e. all words in s have initial capitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick examples:\n",
    "word = \"hello2\"\n",
    "word.startswith('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word.endswith('e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word.endswith('lo2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2 = 'hell'\n",
    "word2 in word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word.islower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num='839'\n",
    "num.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word3 = \"Alvid\"\n",
    "word3.istitle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word3.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find hyphenated words with specific text\n",
    "sorted(w for w in set(text7) if '-' in w and 'down' in w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for variations of a words:\n",
    "sorted(w for w in set(text1) if w.startswith('run') or w.startswith('ran'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all upper case words:\n",
    "[word.upper() for word in text1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
